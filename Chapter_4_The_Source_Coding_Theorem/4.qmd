---
title: "4. The Source Coding Theorem"
format:
  revealjs:
    toc: true
    smaller: true
    scrollable: true

---

## 4.1 How to measure the information content of a random variable?
*Reminders:*

An ensemble X is a denoted by ($x, A_X, P_X$), where the outcome $x$ is the value of a random variable, which takes one of a set of possible values $A_X = \{a_1, a_2, \dots\}$ having probabilites $P_X = \{p_1, p_2, \dots \}$ with $P(x=a_i) = p_i$, $p_i \geq 0$ and $\sum_{a_i \in A_X} P(x=a_i) = 1$

Shannon information content:
$$ h(x=a_i) \equiv \log_{2} \frac{1}{p_i} $$

Entropy of the ensemble:
$$ H(X) = \sum_i p_i\log_{2} \frac{1}{p_i} $$

## 
### Information content of independent random variables

> Why are we using $\log 1/p_i$ to measure information content?

The definition of independence is that the probability distribution is separable into a product:

$$P(x,y) = P(x)P(y)$$

Good thing about $\log$ functions is the additive property of profucts of varables. So or independent random variables $x$ and $y$, the information gained when we learn $x$ and $y$ should equal the sum of the information gained if $x$ alone were learned and the information gained if $y$ alone were learned.

##
The Shannon information content of the outcome $x$, $y$ is

$$h(x,y) = \log\frac{1}{P(x,y)} = \log\frac{1}{P(x)P(y)} = \log\frac{1}{P(x)} + \log \frac{1}{P(y)}$$ 

so if $x$ and $y$ are independent,

$$h(x,y) = h(x) + h(y)$$

Same can be shown for entropy of independent $x,y$:

$$H(X,Y) = H(X) + H(Y)$$

##
### Guessing games
In the game of twenty questions, one player thinks of an object, and the other player attempts to guess what the object is by asking questions that have yes/no answers, for example, ‘is it alive?’, or ‘is it human?’ The aim is to identify the object with as few questions as possible. What is the best strategy for playing this game? For simplicity, imagine that we are playing the rather dull version of twenty questions called ‘sixty-three’.

What’s the smallest number of yes/no questions needed to identify an integer x between 0 and 63?

##
Intuitively, the best questions successively divide the 64 possibilities into equal sized sets. Six questions suﬃce. One reasonable strategy asks the following questions:

1. is $x\geq 32$?
2. is $x\mod 32 \geq 16$?
3. is $x\mod 16 \geq 8$?
4. is $x\mod 8 \geq 4$?
5. is $x\mod 4 \geq 2$?
6. is $x\mod 2 = 1$?

The answers to these questions, if translated from $\{yes,no\}$ to $\{1,0\}$, give the binary expansion of $x$, for example $35 \rightarrow 100011$.

## 
If we assume that all values are equally likely, and answers to the quastions are independent, so each has Shannon inforamtion content $\log_{2} (1/0.5) = 1$ bit; total Shannon information is always six bits. 

We can view this as a way of encoding x as a binary code.

>Does Shannon information content make sense for ensembles where the ourcomes have unequal proabilites?

##
### The game of submarine

In this game *submarine*, a simplified and boring version of battleships, each player hides one single submarine on an 8×8 grid. Players take turns guessing a square (like “G3”). The opponent replies with miss, hit, or hit and destroyed. Since there is only one submarine, the game ends as soon as that one square is hit.

```{python}
import matplotlib.pyplot as plt

fired_at = (3, 4)       
misses = [(1, 1), (2, 5), (7, 3)]   
hit = (6, 2)            

fig, ax = plt.subplots(figsize=(4, 4))

for i in range(9):
    ax.axhline(i, linewidth=1)
    ax.axvline(i, linewidth=1)

ax.scatter(fired_at[1]-0.5, fired_at[0]-0.5, s=500, facecolors='none')

for r, c in misses:
    ax.text(c-0.5, r-0.5, '×', ha='center', va='center', fontsize=20)

ax.text(hit[1]-0.5, hit[0]-0.5, 's', ha='center', va='center', fontsize=20)

ax.set_xlim(0, 8)
ax.set_ylim(8, 0)
ax.set_xticks(range(9))
ax.set_yticks(range(9))
ax.set_aspect('equal')
plt.show()

```


the circle represents the square that is being fired at, and the ×s show squares in which the outcome was a miss, $x= n$; thesubmarine is hit $x= y$ shown by the symbol $s$


##
Each shot made by a player defines an ensemble. The two possible outcomes are $\{y,n\}$, corresponding to a hit and a miss, and their probabilities depend on the state of the board. At the beginning, $P(y) = 1/64$ and $P(n) = 63/64$. At the second shot, if the first shot missed, $P(y) = 1/63$ and
$P(n) = 62/63$. At the third shot, if the first two shots missed, $P(y) = 1/62$
and $P(n) = 61/62$.

If we hit the submarine in the first shot, 
$$h(x) = h_{(1)}(y) = \log_{2} 64 = 6 \text{ bits.} $$

We have learnt the hiding place, which could have been any of 64 squares; so we have, by one lucky binary question, indeed learnt six bits. 

## 
What if the first shot misses? The Shannon information gained is
$$
h_{(1)} = \log_{2}\frac{64}{63} = 0.0227 \text{ bits}. 
$$

If the second shot also misses, the information gained is
$$
h_{(2)} = \log_{2}\frac{63}{62} = 0.0230 \text{ bits}. 
$$

After missing 32 times (each at a new square), the total Shannon information is
$$
\log_{2}\!\left(\frac{64}{63}\right)
+ \log_2\!\left(\frac{63}{62}\right)
+ \cdots
+ \log_2\!\left(\frac{33}{32}\right)
= 0.0227 + 0.0230 + \cdots + 0.0430
= 1.0 \text{ bits}. 
$$

This answer rules out half of the hypotheses, so it gives us one bit.

##

If we hit the submarine on the 49th shot, with 16 squares left, the information gained is
$$
h_{(49)}(y)=\log_2 16 = 4.0 \text{ bits}.
$$

The total Shannon information of all outcomes is
$$
\log_2\!\left(\frac{64}{63}\right)
+ \log_2\!\left(\frac{63}{62}\right)
+ \cdots
+ \log_2\!\left(\frac{17}{16}\right)
+ \log_2\!\left(\frac{16}{1}\right)
= 0.0227 + 0.0230 + \cdots + 0.0874 + 4.0
= 6.0 \text{ bits}. 
$$

Thus, once the submarine is located, the total information gained is 6 bits.

More generally, if the hit occurs when $n$ squares remain, the information is
$$
\log_2\!\left(\frac{64}{63}\right)
+ \log_2\!\left(\frac{63}{62}\right)
+ \cdots
+ \log_2\!\left(\frac{n+1}{n}\right)
+ \log_2\!\left(\frac{n}{1}\right)
= \log_2\!\left(\frac{64}{1}\right)
= 6 \text{ bits}. 
$$

##
### Wenglish

Wenglish uses 5-letter "words" chosen uniformly at random from a fixed dictionary of exactly $32{,}768 = 2^{15}$ words. Each word in the dictionary was originally generated by drawing five letters independently (the same distribution as real English).
Key observed statistics:

Probability of letter a ≈ $0.0625$, so ≈ $2048$ dictionary words start with a (among them, $128$ start with aa, $2$ with az).
Probability of letter z ≈ $0.001$, so only $32$ words start with z.

Shannon Information Content
When reading one full word at a time (uniform over the $32{,}768$ words):
$$H(\text{word}) = \log_2 32{,}768 = 15~\text{bits}$$
Average per character: $15 / 5 = 3$ bits.

##
When reading one character at a time:

If the first letter is a:
$$I(a) = -\log_2 0.0625 \approx 4~\text{bits}$$
If the first letter is z:
$$I(z) = -\log_2 0.001 \approx 10~\text{bits}$$

Thus, the information conveyed by each character is highly variable and depends strongly on position. Rare initial letters (like z) carry much more information than common ones (like a), because they narrow down the possible words far more quickly.

This mirrors real English: words beginning with rare letters (e.g., “xylophone”) are identified almost immediately, while words starting with common prefixes (e.g., “pro…”) require many more letters to disambiguate.


## 4.2 Data compression
The preceding examples justify the idea that the Shannon information content of an outcome is a natural measure of its information content. We now discuss the information content of a source by considering how many bits are needed to describe the outcome of an experiment.

If we can show that we can compress data from a particular source into a file of L bits per source symbol and recover the data reliably, then we will say that the average information content of that source is at most L bits per symbol.

##
### Compression of text files
A file is composed of a sequence of bytes. A byte is composed of 8 bits and can have a decimal value between 0 and 255. A typical text file is composed of the ASCII character set (decimal values 0 to 127). This character set uses only seven of the eight bits in a byte.

>By how much could the size of a file be reduced given that it is an ASCII file? How would you achieve this reduction?

##
Intuitively, we may get rid of the redundant bit in each character, reducing the file size by 7/8. 

Most sources of data have further redundancy: English text files use the ASCII characters with non-equal frequency; certain pairs of letters are more probable than others; and entire words can be predicted given the context and a semantic understanding of the text.

One simple way to measure the information in a random variable is to count how many outcomes it can take. If the set of possible outcomes is $A_X$, then $|A_X|$ is the number of outcomes.

##

If we give each outcome a binary label, each label needs
$$
\log_2 |A_X|
$$
bits (when $|A_X|$ is a power of 2). This leads to the definition:

$$
H_0(X) = \log_2 |A_X|
$$

This is the *raw bit content* of $X$. It tells us the minimum number of yes/no questions needed to always determine the outcome.

It is additive: if $X$ and $Y$ are two variables, then the pair $(X, Y)$ has $|A_X||A_Y|$ possible outcomes, so
$$
H_0(X, Y) = H_0(X) + H_0(Y).
$$

This measure ignores probabilities and does not compress data — it simply assigns every outcome a binary string of fixed length.


##
Question:

Could there be a compressor that maps an outcome $x$ to a binary code $c(x)$, and a decompressor that maps $c$ back to $x$, such that every possible outcome is compressed into a binary code of length shorter than $H_0(X)$ bits?

##
Answer: No

The pigeon-hole principle states: you can’t put 16 pigeons into 15 holes without using one of the holes twice.Similarly, you can’t give $A_X$ outcomes unique binary names of some length $l$ shorter than $\log_{2} |A_X|$ bits.

##

There are only two ways in which a ‘compressor’ can actually compress
files:

1. A lossy compressor compresses some files, but maps some files to thesame encoding. We’ll assume that the user requires perfect recovery ofthe source file, so the occurrence of one of these confusable files leads to a failure (though in applications such as image compression, lossy compression is viewed as satisfactory). We’ll denote by $\delta $the probability that the source string is one of the confusable files, so a lossy compressor has a probability $\delta$ of failure. If $\delta$ can be made very small then a lossy compressor may be practically useful.

2. A lossless compressor maps all files to diﬀerent encodings; if it shortens some files, it necessarily makes others longer. We try to design the compressor so that the probability that a file is lengthened is very small, and the probability that it is shortened is large.

This chapter only covers lossy compressors

## 4.3 Information Content via Lossy Compression

Instead of assigning names to all possible outcomes, we can tolerate a small failure probability $\delta$ and dramatically reduce the number of bits needed. The key insight: not all outcomes are equally likely, so we can ignore improbable ones.

For an 8-symbol alphabet:
$$
\begin{align}
A_X &= \{a, b, c, d, e, f, g, h\}, \\
P_X &= \left\{\tfrac14, \tfrac14, \tfrac14, \tfrac{3}{16},
        \tfrac{1}{64}, \tfrac{1}{64}, \tfrac{1}{64}, \tfrac{1}{64}\right\}.
\end{align}
$$

- With $\delta = 0$: need 3 bits (all 8 symbols)
- With $\delta = 1/16$: need only 2 bits (top 4 symbols have probability 15/16)


##


We rank all outcomes by decreasing probability and include only the most probable ones until their cumulative probability reaches $1-\delta$. This creates the *smallest $\delta$-sufficient subset* $S_\delta$.

The *essential bit content* is defined as:
$$H_\delta(X) = \log_2 |S_\delta|$$

![](neccesarry_files/hvsdelta.png)

##
### Extended Ensembles

For sequences of $N$ independent symbols from ensemble $X$, we denote this as $X^N$. The entropy scales linearly: $H(X^N) = NH(X)$.

Binary example with $p_1 = 0.1, p_0 = 0.9$:

For $N=4$, $H_\delta(X^4)$ varies significantly with $\delta$. But as $N$ increases, something remarkable happens: $\frac{1}{N}H_\delta(X^N)$ becomes nearly flat across different values of $\delta$, hovering around $H(X) \approx 0.47$ bits per symbol.

![](neccesarry_files/h4func.png)
![](neccesarry_files/hxfunc.png)

##

This convergence leads directly to Shannon's source coding theorem.

*Shannon's source coding theorem:* Let $X$ be an ensemble with entropy $H(X) = H$ bits. Given $ \varepsilon > 0$ and $0 < \delta < 1$, there exists a positive integer $N_0$ such that for $N > N_0$,

$$\left|\frac{1}{N}H_\delta(X^N) - H\right| <  \varepsilon.$$

## 4.4 Typicality

Consider a binary sequence of length $N=100$ with $p_1 = 0.1$. Most sequences contain approximately 10 ones (within about $\pm 3$). The probability of a sequence with $r$ ones is:
$$P(x) = p_1^r(1-p_1)^{N-r}$$

As the N gets larger, the probability distribution of $r$ becomes more concentrated, since the range of possile values of $r$ grows as $N$ but the standard deviation grows only as $\sqrt{N}$

Since r likely falls in a small range, x also likely falls in a corresponding small subset of outcomes called *the typical set*.

##
```{python}
import numpy as np
import matplotlib.pyplot as plt

# Parameters
N1 = 100
N2 = 1000
p = 0.1

def normal_pdf(x, mu, sigma):
    """Calculates the PDF for a normal distribution."""
    coefficient = 1.0 / (sigma * np.sqrt(2.0 * np.pi))
    exponent = -0.5 * ((x - mu) / sigma)**2
    return coefficient * np.exp(exponent)

# --- Plot 1: N = 100 ---
mu_100 = N1 * p
sigma_100 = np.sqrt(N1 * p * (1 - p))

# Define the range for the x-axis (0 to 100)
x1 = np.linspace(0, N1, 500)
pdf_100 = normal_pdf(x1, mu=mu_100, sigma=sigma_100)

plt.figure(figsize=(10, 6))
plt.plot(x1, pdf_100, label=f'$N={N1}$, $\\mu={mu_100:.0f}$, $\\sigma={sigma_100:.2f}$')
plt.axvline(mu_100, color='r', linestyle='--', alpha=0.7, label='Mean ($\mu=10$)')

plt.title(f'Normal Approximation for Number of Successes ($k$) with $N={N1}$ ($p = {p}$)')
plt.xlabel('Number of Successes ($k = n(r)$)')
plt.ylabel('Probability Density')
plt.xlim(0, N1)
plt.legend()
plt.grid(True, linestyle='--', alpha=0.6)

plt.show()


# --- Plot 2: N = 1000 ---
mu_1000 = N2 * p
sigma_1000 = np.sqrt(N2 * p * (1 - p))

# Define the range for the x-axis (0 to 1000)
x2 = np.linspace(0, N2, 500)
pdf_1000 = normal_pdf(x2, mu=mu_1000, sigma=sigma_1000)

plt.figure(figsize=(10, 6))
plt.plot(x2, pdf_1000, label=f'$N={N2}$, $\\mu={mu_1000:.0f}$, $\\sigma={sigma_1000:.2f}$')
plt.axvline(mu_1000, color='r', linestyle='--', alpha=0.7, label='Mean ($\mu=100$)')

plt.title(f'Normal Approximation for Number of Successes ($k$) with $N={N2}$ ($p = {p}$)')
plt.xlabel('Number of Successes ($k = n(r)$)')
plt.ylabel('Probability Density')
plt.xlim(0, N2)
plt.legend()
plt.grid(True, linestyle='--', alpha=0.6)

plt.show()
```

##
For a "typical" sequence with $r \approx 10$:
$$\log_2 \frac{1}{P(x)} \approx N \sum_i p_i \log_2 \frac{1}{p_i} = NH$$

So, the information content of **$x$** is very likely to be close in value to $NH$

##
### The Typical Set $T_{N\beta}$
We define typical sequences as those whose probability is close to $2^{-NH}$, and introsuce a parameter $\beta$ that determines how close the probability should be to be to $2^{-NH}$ in the typical set:
$$T_{N\beta} = \left\{x : \left|\frac{1}{N}\log_2\frac{1}{P(x)} - H\right| < \beta\right\}$$

##
### Asymptotic Equipartition Principle
For an ensemble of $N$ independent identically distributed (i.i.d.) random variables $X^N \equiv (X_1,X_2, \dots ,X_N)$, with $N$ suﬃciently large, the outcome $x = (x_1,x_2,\dots ,x_N)$ is almost certain to belong to a subset of $A^N_X$ having only $2^{NH(X)}$ members, each having probability ‘close to’ $2^{−NH(X)}$

Imagine all $2^{NH_0}$ possible sequences ranked by probability. The typical set $T_{N\beta}$ occupies a narrow band around probability $2^{-NH}$, yet contains nearly all the probability mass.

---

## 4.5 Proofs

### The Law of Large Numbers

$$\begin{align} \text{Mean:} &\quad  \varepsilon[u] = \bar{u} = \sum_u P(u)u \\
\text{Variance:} &\quad \mathrm{var}(u) = \sigma_u^2 = \varepsilon[(u- \bar{u})^2] =  \sum_u P(u)(u- \bar{u})^2 \end{align} 
$$


**Chebyshev's inequality 1:** 
Let $t$ be a non-negative real random variable, and let $\alpha$ be a positive real number. Then:

$$
P(t\geq \alpha) \leq \frac{\bar{t}}{\alpha}
$$


##
Proof: $P(t \geq \alpha) = \sum_{t \geq \alpha} P(t)$. We multiply each term by $t/\alpha \geq 1$ and obtain: $P(t \geq \alpha) \leq \sum_{t \geq \alpha} P(t)t/\alpha$. We add the (non-negative) missing terms and obtain: $P(t \geq \alpha) \leq \sum_t P(t)t/\alpha = \bar{t}/\alpha$.

If we put $t=(x-\bar{x})^2$ we get:

$$P((x−\bar{x})^2 \geq \alpha) \leq \sigma^2_x /\alpha$$

**Weak law:** For the average $x = \frac{1}{N}\sum_{n=1}^N h_n$ of $N$ independent random variables $h_1,\dots,h_N$ with common mean $\bar{h}$ and variance $\sigma_h^2$:
$$P((x-\bar{h})^2 \geq \alpha) \leq \frac{\sigma_h^2}{\alpha N}$$

Proof: obtained by showing that $\bar{x}= \bar{h}$ and that $\sigma^2_x = \sigma^2_h/N$

No matter how small we want the deviation $\alpha$, we can make the probability arbitrarily small by choosing $N$ large enough.

##
### Proof of Source Coding Theorem

We apply the law of large numbers to $\frac{1}{N}\log_2\frac{1}{P(x)}$, which has mean $H$ and variance $\sigma^2$.

**Part 1:** The Upper Bound
Show that
$$
\frac{1}{N}H_\delta(X^N) < H + \varepsilon.
$$

Let the typical set be
$$
T_{N,\beta} = \left\{ x^N : \left| \frac{1}{N}\log_{2}\!\left(\frac{1}{P(x^N)}\right) - H \right| < \beta \right\}
$$


For every $x^N \in T_{N,\beta}$,
$$
2^{-N(H+\beta)} < P(x^N) < 2^{-N(H-\beta)}.
$$

##
Because the total probability of $T_{N,\beta}$ cannot exceed $1$,
$$
|T_{N,\beta}| , 2^{-N(H+\beta)} < 1,
$$
so
$$
|T_{N,\beta}| < 2^{N(H+\beta)}.
$$

Set $\beta = \varepsilon$. Choose $N$ large enough that
$$
P(T_{N,\beta}) \ge 1 - \delta.
$$

Since $T_{N,\beta}$ is a $(1-\delta)$-sufficient set,
$$
H_\delta(X^N) \le \log_2|T_{N,\beta}| < N(H + \varepsilon),
$$
which implies
$$
\frac{1}{N}H_\delta(X^N) < H + \varepsilon.
$$

##
**Part 2:** Lower Bound 

Show that
$$
\frac{1}{N}H_\delta(X^N) > H - \varepsilon.
$$

Let $\beta = \varepsilon/2$. Suppose, for contradiction, that there exists a $(1-\delta)$-sufficient set $S'$ with
$$
|S'| \le 2^{N(H - 2\beta)} = 2^{N(H - \varepsilon)}.
$$

Decompose its probability:
$$
P(S') = P(S' \cap T_{N,\beta}) + P(S' \cap T_{N,\beta}^c).
$$

Inside the typical set, every sequence has probability at most $2^{-N(H-\beta)}$, so
$$
P(S' \cap T_{N,\beta}) \le |S'| , 2^{-N(H-\beta)}
\le 2^{N(H-2\beta)} , 2^{-N(H-\beta)}
= 2^{-N\beta}.
$$


##
The second term is bounded by the LLN/AEP deviation bound:
$$
P(T_{N,\beta}^c) \le \frac{\sigma^2}{\beta^2 N}.
$$

Thus
$$
P(S') \le 2^{-N\beta} + \frac{\sigma^2}{\beta^2 N}.
$$

The right-hand side tends to $0$ as $N\to\infty$, so for sufficiently large $N$,
$$
P(S') < 1 - \delta,
$$
contradicting the assumption that $S'$ is $(1-\delta)$-sufficient.

Therefore, for all sufficiently large $N$:
$$
H_\delta(X^N) > N(H - \varepsilon),
$$
and hence
$$
\frac{1}{N}H_\delta(X^N) > H - \varepsilon.
$$